[English](readme.md) | [ä¸­æ–‡](readme_cn.md)

<div align="center">

<img src="STaR.png" alt="STaR Logo" width="420"/>

# â­ STaR: Towards Effective and Stable Table Reasoning via Slow-Thinking Large Language Models

[![arXiv](https://img.shields.io/badge/arXiv-2511.11233-b31b1b.svg?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2511.11233)
[![Hugging Face Datasets](https://img.shields.io/badge/ğŸ¤—%20Datasets-STaR--Datasets-yellow?style=flat-square)](https://huggingface.co/datasets/zhjai/STaR-Datasets)
[![License](https://img.shields.io/badge/License-MIT-green.svg?style=flat-square)](LICENSE)

**A Novel Slow-Thinking Model for Effective and Stable Table Reasoning**

[ğŸ“„ Paper](https://arxiv.org/abs/2511.11233) â€¢ [ğŸ¤— Datasets](https://huggingface.co/datasets/zhjai/STaR-Datasets) â€¢ [ğŸ  GitHub](https://github.com/zhjai/STaR)

</div>

---

## ğŸ“Œ Overview

<div align="center">
<img src="star-framework.png" alt="STaR Framework" width="90%"/>
</div>

**STaR** (Slow-Thinking Table Reasoning) is a novel slow-thinking model that can achieve effective and stable table reasoning. It enables effective multi-step reasoning through a two-stage training framework (SFT + RFT) and improves reasoning stability via trajectory-level uncertainty quantification.

### âœ¨ Key Features

- ğŸ§  **Effective Multi-Step Reasoning**: Two-stage training framework with SFT warm-up and reinforced fine-tuning (RFT)
- ğŸ“ˆ **Difficulty-Aware RL**: Reinforcement learning mechanism that progressively handles complex reasoning
- ğŸ¯ **Stable Reasoning**: Trajectory-level uncertainty quantification fusing token-level confidence with answer-level consistency
- ğŸš€ **Strong Generalization**: State-of-the-art in-domain performance and excellent out-of-domain generalization

---

## ğŸ“‹ Abstract

Table reasoning with large language models (LLMs) plays a critical role in building intelligent systems capable of understanding and analyzing tabular data. Despite recent progress, existing methods still face key limitations: their reasoning processes lacks depth and explicit multi-step reasoning, often relying solely on implicit language model understanding. In addition, their reasoning processes suffer from instability, primarily caused by model uncertainty.

In this work, we propose **STaR**, a novel slow-thinking model that can achieve effective and stable table reasoning. To enable effective multi-step reasoning, we design a **two-stage training framework** consisting of supervised fine-tuning (SFT) warm-up followed by reinforced fine-tuning (RFT). Specifically, in the SFT stage, we construct a high-quality dataset through automatic self-verification. In the RFT stage, we introduce a **difficulty-aware reinforcement learning mechanism** to further enhance reasoning capabilities. Furthermore, to improve reasoning stability, we introduce **trajectory-level uncertainty quantification**, which fuses token-level confidence with answer-level consistency, enabling the selection of better reasoning trajectories. Extensive experiments demonstrate that STaR-8B achieves state-of-the-art performance on in-domain benchmarks and exhibits strong generalization to out-of-domain datasets, highlighting its potential for enhancing both effectiveness and stability in table reasoning.

---

## ğŸ“ Project Structure

```
STaR/
â”œâ”€â”€ ğŸ“‚ data/                    # Datasets
â”‚   â”œâ”€â”€ STaR-sft.parquet        # SFT training data
â”‚   â”œâ”€â”€ STaR-train-easy.parquet # Easy training samples
â”‚   â”œâ”€â”€ STaR-train-hard.parquet # Hard training samples
â”‚   â”œâ”€â”€ STaR-train-all.parquet  # All training samples
â”‚   â”œâ”€â”€ STaR-eval.parquet       # Evaluation data
â”‚   â””â”€â”€ STaR-test.parquet       # Test data
â”œâ”€â”€ ğŸ“‚ model/                   # Pre-trained models
â”œâ”€â”€ ğŸ“‚ sh/                      # Training & evaluation scripts
â”œâ”€â”€ ğŸ“‚ verl/                    # VERL framework
â”œâ”€â”€ ğŸ“‚ checkpoints/             # Model checkpoints
â”œâ”€â”€ ğŸ“„ reward.py                # Reward function
â”œâ”€â”€ ğŸ“„ eval-by-trajectory.py    # Evaluation script
â””â”€â”€ ğŸ“„ requirements.txt         # Dependencies
```

---

## ğŸ› ï¸ Installation

> **Requirements**: Python 3.10+ and CUDA GPUs

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/zhjai/STaR.git
cd STaR

# 2ï¸âƒ£ Install Python dependencies
pip install -r requirements.txt

# 3ï¸âƒ£ Install verl in editable mode
cd verl
pip install -e .
cd ..
```

---

## ğŸ“¦ Data & Models

### ğŸ¤— Datasets

Download the datasets from Hugging Face and place them in the `data/` folder:

| Dataset | Description | Link |
|---------|-------------|------|
| STaR-Datasets | Full training & evaluation data | [![Hugging Face](https://img.shields.io/badge/ğŸ¤—-Datasets-yellow)](https://huggingface.co/datasets/zhjai/STaR-Datasets) |

### ğŸ¤– Base Models

Download the base models and place them in the `model/` folder:

| Model | Parameters | Link |
|-------|------------|------|
| Qwen3-0.6B | 0.6B | [![Hugging Face](https://img.shields.io/badge/ğŸ¤—-Qwen3--0.6B-yellow)](https://huggingface.co/Qwen/Qwen3-0.6B) |
| Qwen3-8B | 8B | [![Hugging Face](https://img.shields.io/badge/ğŸ¤—-Qwen3--8B-yellow)](https://huggingface.co/Qwen/Qwen3-8B) |

### ğŸ† Trained Checkpoints

Our trained model weights are available on Hugging Face:

| Model | Parameters | Link |
|-------|------------|------|
| STaR-0.6B | 0.6B | [![Hugging Face](https://img.shields.io/badge/ğŸ¤—-STaR--0.6B-yellow)](https://huggingface.co/zhjai/STaR-0.6B) |
| STaR-8B | 8B | [![Hugging Face](https://img.shields.io/badge/ğŸ¤—-STaR--8B-yellow)](https://huggingface.co/zhjai/STaR-8B) |

---

## ğŸš€ Training

Training scripts are located in the `sh/` directory. Adjust paths and hyperparameters as needed.

### ğŸ“š Stage 1: Supervised Fine-Tuning (SFT)

```bash
# Qwen3-0.6B
bash sh/STaR-sft-qwen3-0.6b.sh

# Qwen3-8B
bash sh/STaR-sft-qwen3-8b.sh
```

### ğŸ¯ Stage 2: Reinforced Fine-Tuning (RFT) - Foundational Training

```bash
# Qwen3-0.6B
bash sh/STaR-sft-stage1-qwen3-0.6b.sh

# Qwen3-8B
bash sh/STaR-sft-stage1-qwen3-8b.sh
```

### ğŸ”¥ Stage 2: Reinforced Fine-Tuning (RFT) - Progressive Training

```bash
# Qwen3-0.6B
bash sh/STaR-sft-stage1-stage2-qwen3-0.6b.sh

# Qwen3-8B
bash sh/STaR-sft-stage1-stage2-qwen3-8b.sh
```

---

## ğŸ“Š Evaluation

### 1ï¸âƒ£ Generate Trajectories

```bash
bash sh/STaR-eval.sh
```

### 2ï¸âƒ£ Compute Metrics

```bash
python eval-by-trajectory.py
```

---

## ğŸ“– Citation

If you find this work useful, please cite our paper:

> **Note:** The citation on Google Scholar may still display the old title. The correct title is: *STaR: Towards Effective and Stable Table Reasoning via Slow-Thinking Large Language Models*

---

## ğŸ™ Acknowledgements

- This work builds on the excellent [**VERL**](https://github.com/volcengine/verl) framework
- Base models from [**Qwen**](https://github.com/QwenLM/Qwen) team at Alibaba
- Thanks to the open-source community for tools and datasets

---

<div align="center">

**â­ Star this repo if you find it helpful! â­**

Made with â¤ï¸ for the research community

</div>
